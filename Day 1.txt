Day 1:

what is Rag.

What is vector.

what is a embedding model.

what is ollama.


whats is cosine and Euclidian distance.

Chroma Database is where you vector embeddings were stored.

Important : System environment variables.


Embedding Code Snipet:
from transformers import AutoTokenizer, AutoModel
import torch

# Load pre-trained model and tokenizer
model_name = "distilbert-base-uncased"  # You can use other models too, e.g., 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def word_to_vector(word):
    # Tokenize input word
    inputs = tokenizer(word, return_tensors="pt", padding=True, truncation=True)
    
    # Get model outputs
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Get the embeddings of the word (first token of the output)
    word_vector = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # Averaging over the tokens
    return word_vector

# Example usage
word = "what is the use of program data"
vector = word_to_vector(word)
print(f"Vector for the word '{word}':\n{vector}")



ml@pop-os:~$ curl -X POST http://localhost:11434/api/chat -H "Content-Type: application/json" -d '{
   "model": "llama3.2:3b",
   "messages": [
     {
       "role": "user",
       "content": "What is the capital of france?"
     }
   ]
}'
{"model":"llama3.2:3b","created_at":"2024-12-02T06:51:02.147439431Z","message":{"role":"assistant","content":"The"},"done":false}
{"model":"llama3.2:3b","created_at":"2024-12-02T06:51:02.23235997Z","message":{"role":"assistant","content":" capital"},"done":false}
{"model":"llama3.2:3b","created_at":"2024-12-02T06:51:02.317344812Z","message":{"role":"assistant","content":" of"},"done":false}
{"model":"llama3.2:3b","created_at":"2024-12-02T06:51:02.402500196Z","message":{"role":"assistant","content":" France"},"done":false}
{"model":"llama3.2:3b","created_at":"2024-12-02T06:51:02.487580605Z","message":{"role":"assistant","content":" is"},"done":false}
{"model":"llama3.2:3b","created_at":"2024-12-02T06:51:02.572626874Z","message":{"role":"assistant","content":" Paris"},"done":false}
{"model":"llama3.2:3b","created_at":"2024-12-02T06:51:02.657625233Z","message":{"role":"assistant","content":"."},"done":false}
{"model":"llama3.2:3b","created_at":"2024-12-02T06:51:02.742829609Z","message":{"role":"assistant","content":""},"done_reason":"stop","done":true,"total_duration":1182003622,"load_duration":64282412,"prompt_eval_count":32,"prompt_eval_duration":517000000,"eval_count":8,"eval_duration":598000000}



Local Ollama API calls:
import requests
import json

class OllamaLLM:
    def __init__(self, url="http://localhost:11434/api/chat"):
        self.url = url

    def generate(self, prompt):
        data = {
            "model": "llama3.2:3b",
            "messages": [{"role": "user", "content": prompt}]
        }
        response = requests.post(self.url, json=data, headers={"Content-Type": "application/json"}, stream=True)
        accumulated_content = ""
        for chunk in response.iter_lines(decode_unicode=True):
            if chunk:
                try:
                    json_data = json.loads(chunk)
                    if 'message' in json_data:
                        accumulated_content += json_data['message']['content']
                    if json_data.get('done', False):
                        break
                except json.JSONDecodeError as e:
                    print(f"Error decoding chunk: {e}")
        return accumulated_content

def conversational_example():
    ollama_llm = OllamaLLM()
    prompt1 = "Who was Isaac Newton?"
    response1 = ollama_llm.generate(prompt1)
    print(f"Q1: {prompt1}")
    print(f"A1: {response1}\n")

conversational_example()


# Call the function
if __name__ == "__main__":
    conversational_example()


Gorq Api Calls: For Text
from groq import Groq

# Pass the API key directly
client = Groq(
    api_key="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"
)

# Create a chat completion request
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Who was Isaac Newton?",
        }
    ],
    model="llama-3.2-3b-preview",
)

# Print the response from the API
print(chat_completion.choices[0].message.content)


Gorq Api Call : For Image:

from groq import Groq
import base64


# Function to encode the image
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

# Path to your image
image_path = "C:\\Rag-app\\01_chest.jpg"

# Getting the base64 string
base64_image = encode_image(image_path)

client = Groq(api_key="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")

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What's in this image? any abnormalty found in this ?"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}",
                    },
                },
            ],
        }
    ],
    model="llama-3.2-90b-vision-preview",
)

print(chat_completion.choices[0].message.content)









